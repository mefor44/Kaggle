{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I will work with Titanic data. This time I'm going to build ensemble stacking machinery based on few machine learning algoriths like random forest, SVM, k-nearest neighbours and SGD classifier. No neural network will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from my_own_transformers.ipynb\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.api import OLS\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import import_ipynb\n",
    "import my_own_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n",
      "None\n",
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "\n",
      "                                                Name     Sex   Age  SibSp  \\\n",
      "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "4                           Allen, Mr. William Henry    male  35.0      0   \n",
      "\n",
      "   Parch            Ticket     Fare Cabin Embarked  \n",
      "0      0         A/5 21171   7.2500   NaN        S  \n",
      "1      0          PC 17599  71.2833   C85        C  \n",
      "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "3      0            113803  53.1000  C123        S  \n",
      "4      0            373450   8.0500   NaN        S  \n",
      "MIssing values:\n",
      " PassengerId      0\n",
      "Survived         0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age            177\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             0\n",
      "Cabin          687\n",
      "Embarked         2\n",
      "dtype: int64\n",
      "MIssing values:\n",
      " PassengerId      0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age             86\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             1\n",
      "Cabin          327\n",
      "Embarked         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from my_own_transformers import load_and_present, filling_values_regression\n",
    "\n",
    "\n",
    "data = load_and_present(\"train.csv\")\n",
    "submission_data =  load_and_present(\"test.csv\", head=False, info=False)\n",
    "\n",
    "random_state=14\n",
    "LEN = len(data)\n",
    "COLUMNS_TO_DROP = []\n",
    "\n",
    "\n",
    "pass_id = submission_data[\"PassengerId\"]#will be used later\n",
    "data.drop([\"PassengerId\", \"Name\",\"Ticket\",\"Cabin\"],axis=1,inplace=True)\n",
    "submission_data.drop([\"PassengerId\", \"Name\",\"Ticket\",\"Cabin\"],axis=1,inplace=True)\n",
    "\n",
    "\n",
    "#missing values easiest treatment\n",
    "data[\"Embarked\"].fillna(inplace=True,value=\"Q\")\n",
    "submission_data[\"Fare\"].fillna(inplace=True,value=submission_data[\"Fare\"].mean())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have already done EDA (exploratory data analysis) in previous notebook. The only thing I will add before going into building and tesing models are combination of features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "my_own_transformers.ipynb:56: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"        output = pd.concat([output,encoded],axis=1)     \\n\",\n"
     ]
    }
   ],
   "source": [
    "#Using pipeline to transform the data\n",
    "from sklearn.pipeline import Pipeline\n",
    "from my_own_transformers import getting_dummies_01, filling_values_regression_method,numerical_to_categorical\n",
    "\n",
    "pipeline_0 = Pipeline(steps=[(\"dummies\", getting_dummies_01(variables_to_dummies=[\"Embarked\", \"Pclass\",\"Sex\"]))])\n",
    "data = pipeline_0.fit_transform(data)\n",
    "submission_data = pipeline_0.fit_transform(submission_data)\n",
    "\n",
    "data = filling_values_regression(data, \"Age\")\n",
    "submission_data = filling_values_regression(submission_data, \"Age\")\n",
    "\n",
    "#Stratified sampling\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=14)\n",
    "for train_index, test_index in split.split(data, data[\"Survived\"]):\n",
    "    train = data.iloc[train_index]\n",
    "    test = data.iloc[test_index]\n",
    "\n",
    "\n",
    "#pipeline v2 \n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "   # (\"dummies\", getting_dummies_01(variables_to_dummies=[\"Embarked\", \"Pclass\",\"Sex\"])),\n",
    "  #  (\"filling_age\", filling_values_regression_method(variable_to_fill=\"Age\",model=LinearRegression(),variable_to_drop=\"Survived\")),\n",
    "    (\"age_to_categorical\", numerical_to_categorical(interval=[10,20,30,40,50,6000],variable=\"Age\")),\n",
    "    (\"dummies_age\", getting_dummies_01(variables_to_dummies=[\"Age\"])),\n",
    "    (\"fare_to_categorical\", numerical_to_categorical(interval=[10,30,6000],variable=\"Fare\")),\n",
    "    (\"dummies_fare\", getting_dummies_01(variables_to_dummies=[\"Fare\"]))\n",
    "])\n",
    "train = pipeline.fit_transform(train)\n",
    "test = pipeline.transform(test)\n",
    "submission_data = pipeline.transform(submission_data)\n",
    "\n",
    "y_train = train[\"Survived\"]\n",
    "x_train = train.drop([\"Survived\"],axis=1)\n",
    "y_test = test[\"Survived\"]\n",
    "x_test = test.drop([\"Survived\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding combination of features (like XY, X^2) - we will not go higher than X^3 for sure, it would be too much features\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "x_train_poly = PolynomialFeatures(degree=2, include_bias=False ).fit_transform(x_train)\n",
    "x_test_poly  = PolynomialFeatures(degree=2, include_bias=False ).fit_transform(x_test)\n",
    "submission_data = PolynomialFeatures(degree=2, include_bias=False ).fit_transform(submission_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea o voting classifier is fitting many different classifiers, like logistic regression, SVM, or random forest and then based on combined informations of their votes (predictions) made final prediction. \n",
    "We can distinguish two ways of voting - soft and hard. Hard voting means looking only on predictions of classifiers contained in voting classifier, in turn soft voting is based on predicted probabilities (it works only if classifiers can calculate the probability)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard features:\n",
      "RandomForestClassifier 0.7988826815642458\n",
      "LogisticRegression 0.7877094972067039\n",
      "SVC 0.7988826815642458\n",
      "VotingClassifier 0.8212290502793296\n",
      "\n",
      "Dataset expanded by combination of features:\n",
      "RandomForestClassifier 0.8156424581005587\n",
      "LogisticRegression 0.8212290502793296\n",
      "SVC 0.7877094972067039\n",
      "VotingClassifier 0.8212290502793296\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "rnf_clf = RandomForestClassifier(n_estimators=200)\n",
    "log_clf = LogisticRegression(max_iter=1000)\n",
    "svc_clf = SVC(probability=True)\n",
    "voting_clf = VotingClassifier(estimators=[(\"01\", rnf_clf), (\"02\", log_clf), (\"03\", svc_clf)],\n",
    "                              voting=\"soft\")\n",
    "\n",
    "print(\"Standard features:\")\n",
    "for clf in (rnf_clf,log_clf,svc_clf,voting_clf):\n",
    "    predictions = clf.fit(x_train,y_train).predict(x_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(predictions,y_test))\n",
    "print(\"\\n\"+\"Dataset expanded by combination of features:\")\n",
    "for clf in (rnf_clf,log_clf,svc_clf,voting_clf):\n",
    "    predictions = clf.fit(x_train_poly,y_train).predict(x_test_poly)\n",
    "    print(clf.__class__.__name__, accuracy_score(predictions,y_test))\n",
    "    \n",
    "#all classifiers hadnt been adjusted (I'm talking about hyperparameters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we can observe that adding more features generally improves the score. I it also worth mentioning that soft voting seems giving better results - which isn't suprising, we are not losing the information of probalility in this approach. Overall voting classifier did better than any of it's sub-classifiers. I'm alsgo going to check the performance of AdaBoostClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard features:\n",
      "AdaBoostClassifier 0.8100558659217877\n",
      "\n",
      "Dataset expanded by combination of features:\n",
      "AdaBoostClassifier 0.8100558659217877\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier()\n",
    "print(\"Standard features:\")\n",
    "predictions = clf.fit(x_train,y_train).predict(x_test)\n",
    "print(ada_clf.__class__.__name__, accuracy_score(predictions,y_test))\n",
    "print(\"\\n\"+\"Dataset expanded by combination of features:\")\n",
    "predictions = clf.fit(x_train_poly,y_train).predict(x_test_poly)\n",
    "print(ada_clf.__class__.__name__, accuracy_score(predictions,y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.76223776 0.78321678 0.85211268 0.81690141 0.80985915]\n",
      "[0.81118881 0.81118881 0.76760563 0.82394366 0.8028169 ]\n",
      "[0.78321678 0.81818182 0.84507042 0.84507042 0.83098592]\n",
      "[0.8041958  0.81818182 0.83098592 0.83802817 0.82394366]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "print(cross_val_score(estimator=AdaBoostClassifier(), X=x_train,y=y_train))\n",
    "print(cross_val_score(estimator=AdaBoostClassifier(), X=x_train_poly,y=y_train))\n",
    "print(cross_val_score(estimator=voting_clf, X=x_train, y=y_train))\n",
    "print(cross_val_score(estimator=voting_clf, X=x_train_poly,y=y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to take care of numbers of features - remove noninformative ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset with important features: (712, 39)\n",
      "[0.8041958  0.84615385 0.81690141 0.83802817 0.81690141]\n"
     ]
    }
   ],
   "source": [
    "ada_clf = ada_clf.fit(x_train_poly,y_train)\n",
    "#pick just the important features\n",
    "indexes = []\n",
    "for i in range(x_train_poly.shape[1]):\n",
    "    if ada_clf.feature_importances_[i]>0:\n",
    "        indexes.append(i)\n",
    "        \n",
    "important_features = x_train_poly[:,indexes]\n",
    "important_features_test = x_test_poly[:,indexes]\n",
    "\n",
    "print(\"Dataset with important features:\", important_features.shape)\n",
    "print(cross_val_score(estimator=voting_clf, X=important_features,y=y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning the parametrs - grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.76923077 0.81118881 0.85915493 0.80985915 0.80985915]\n",
      "AdaBoostClassifier(algorithm='SAMME', base_estimator=None, learning_rate=1,\n",
      "                   n_estimators=100, random_state=None)\n",
      "[0.78321678 0.81118881 0.84507042 0.84507042 0.84507042]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "print(cross_val_score(estimator=AdaBoostClassifier(), X=important_features,y=y_train))\n",
    "\n",
    "\n",
    "param_grid = [{\"n_estimators\": [10,50,100], \"algorithm\": [\"SAMME\", \"SAMME.R\"], \"learning_rate\": [1,0.5]}]\n",
    "\n",
    "grid_search = GridSearchCV(ada_clf, param_grid, cv=5, scoring=\"accuracy\").fit(important_features,y_train)\n",
    "\n",
    "ada_clf_best = grid_search.best_estimator_\n",
    "print(ada_clf_best)\n",
    "print(cross_val_score(estimator=ada_clf_best, X=important_features,y=y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_clf = VotingClassifier(estimators=[(\"01\", rnf_clf), (\"02\", log_clf), (\"03\", svc_clf), (\"04\",ada_clf_best)],\n",
    "                              voting=\"soft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(418, 39)\n",
      "   PassengerId  Survived\n",
      "0          892         0\n",
      "1          893         1\n",
      "2          894         0\n",
      "3          895         0\n",
      "4          896         0\n"
     ]
    }
   ],
   "source": [
    "all_data = np.concatenate((important_features,important_features_test),axis=0)\n",
    "all_y = pd.concat([y_train,y_test],axis=0)\n",
    "voting_clf.fit(all_data, all_y)\n",
    "\n",
    "submission_data = submission_data[:,indexes]\n",
    "print(submission_data.shape)\n",
    "\n",
    "\n",
    "final_submission = pd.DataFrame(data={\"PassengerId\":pass_id,\n",
    "    \"Survived\":[1 if (i > 0.5) else 0 for i in voting_clf.predict(submission_data.reshape(418,-1))]\n",
    "        })\n",
    "print(final_submission.head())\n",
    "final_submission.to_csv(path_or_buf=\"submission_04.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
